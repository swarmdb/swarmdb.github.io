---
layout: post
title: Swarm v 0.4 intro: Technology (2/5)
description: 
modified: 2015-07-16
category: articles
tags: 
image:
  feature: shashlyk.jpg
comments: true
share: true
---

This post continues the 0.4 intro series discussing the basics of the Swarm technology.

## Auto-merged CRDTs

The choice to support both real-time and offline modes was mostly driven by the use case of collaborative apps running on mobile devices. 
It is an unusual ice-and-fire tradeoff as those conditions are perceived as opposites.
But in fact, their respective complexities are caused by exactly the same root cause.
Namely, the asynchrony of the environment, where new events happen faster than other parts of the system become aware of them.
That is how things become difficult.

Swarms deals with asynchrony in the most fundamental way.
Namely, by exclusively using [Commutative Replicated Data Types][crdt].
Those enable both real-time sync (like in Google Docs) and disconnected operation (offline replicas are fully functional). 
CRDT guarantees automatic merge of concurrent changes in an asynchronous environment.
Obviously, that comes at a cost.
For example, the most basic *integer* data type disperses into a variety of integer types, depending on the required behavior: *counter*, *register*, and so on.
But at least, these types have a specified behavior.
An average ad-hoc implementation is guaranteed to be full of surprises once exposed to asynchronous conditions.
Even very professional implementations tend to reveal some [surprises][cc].
Repeatedly.


Hence, Swarm sticks to CRDT.
No manual merge of changes allowed.

On the other hand, if developers wants to, they are free to create their own CRDT data types on top of Swarm POLO layer (partially ordered op log).

Still, CRDT is quite a broad definition.
Those are any types that converge or commute.
So, which ones?

[crdt]: https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type
[cc]: http://www.datastax.com/dev/blog/whats-new-in-cassandra-2-1-a-better-implementation-of-counters

## Op-based CmRDT

Swarm employs op-based Commutative RDTs over state-based Convergent RDTs [server-side implementations][riak] prefer.
Indeed, op-based types are much better for bandwith-restricted environments (client, mobile). 
Op-based CRDTs are somewhat reminiscent of Google's [Operational Transformation][ot]: every change to a data structure is represented as an *operation* (op), which is sent around, applied, etc.
Indeed, imagine if Google Docs would use some state-based approach.
Sending full document state on every keypress would be a suicide.

What is our difference from OT then?
OT operations are repeatedly transformed (by definition) to match the asynchronous environment and its [partial event orders][lamport].
Contrary to that, Swarm CRDT ops are immutable.
Data structures are defined in a way that slight changes in op order do not affect the outcome, as long as cause-effect order is obeyed.

[ot]: http://googledrive.blogspot.com/2010/09/whats-different-about-new-google-docs.html
[cmrdt]: https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type#Operation-based_CRDTs
[riak]: http://docs.basho.com/riak/latest/dev/using/data-types/
[lamport]: https://en.wikipedia.org/wiki/Lamport_timestamps
[smr]: https://en.wikipedia.org/wiki/State_machine_replication

So, every object has multiple replicas, replica states are changed by ops, ops propagate to all the replicas, eventually, to make their states converge.
In the first approach, Swarm is a regular [CmRDT][cmrdt].

Practical question #1.
Suppose, we download a CRDT object.
Should we download its entire op history?
Answer: not in Swarm.
More or less in line with the classic [state machine replication model][smr], new replicas are bootstrapped by serializing their current state and then continuosly updated by a stream of ops.
The key difference from the classic SMR is subtle, but important nevertheless.
SMR assumes a single linear order of all the ops.
We use a partially ordered operation log (POLO); our ops can arrive in different orders to different replicas.

Practical question #2.
In theory, most papers tend to consider "steady state" synchronization.
In practice, steady state sync is a temporary condition, and synchronization can be interrupted halfway.
Especially so in mobile networks.

Hence, Swarm employs sort of a reconciliation handshake protocol.
Two replicas exchange batches of ops to equalize their states and then proceed with steady-state synchronization.
As we deal with partial orders, a handshake is based either on version vectors OR linear positions in the respective op logs.
The latter option is cheaper bandwidth-wise.
Unfortunately, it is also stateful, as the receiver must remember what whas the last op he heard from the sender.

The resulting scheme is nicknamed "a shashlyk":

    0╌╌╌╌┴╌╌┴╌╌╌╌┴╌╌╌╌>

    0  "zero" (default) state
    ╌  ops
    ┴  intermediary states
    >  "head" (recentmost) state

<!-- Fans of streams. Stream of operations (ops)
Difference from Kafka, etc: per-object streams.
Partial order. -->

Differently both from SMR and OT, the accent is on immutable ops.
States are not precisely ephemeral, but the exact state sequence may vary from replica to replica.
Also, your replica might have a state nobody else had.
Still, as all ops reach all the replicas, those converge to the same state

Practical question #3.
How these ops get to those replicas?

## Spanning trees

It is easy to draw up a simple one-to-one client-server sync scenario.
But here we approach the practical question #4.
Is there such a thing as simple client-server sync at all?

First, the client is often intermittently connected and needs to resync on reconnection.
That aspect was covered earlier.
Second, if a relatively simple single-page web app is open in two browser tabs, that creates certain complications.
Some apps simply forbid that, please close the other tab.
Third, a web app is rarely served by a single server.
Multiple servers require server-to-server sync.
Fourth, greater autonomy requires a local cache.
Imagine an office subnet that keeps working when "the cloud" goes down.
Imagine apps that needs to sync within a single mobile device which is currently offline.

Those challenges necessitate a multilevel sync structure: not one-to-one sync. not a "star" but at least a *tree*.
A spanning tree of replicas is our single unifying abstraction for both server-side and client-side sync.
All ops spread by the spanning tree from replica to replica, using the minimal number of transmissions.
That differs from the "gossip" approach that sends updates either in all possible directions or randomly.

Self-stabilizing spanning tree building [algorithms][sst] is a separate interesting topic, so let's skip that.

[sst]: http://infoscience.epfl.ch/record/52545/files/IC_TECH_REPORT_200338.pdf

We assume that for each object, there is a "responsible" server-side storage that is the root of the respective spanning tree.
Everything that gets into the root replica is *truly* saved.
Still, isolated subtrees may function just fine.
Technically, any node may be appointed root. 

So, let's reiterate question #2.
Remember that the graph of replicas is far from steady state, connectivity is intermittent.
Still, we need to propagate all the ops to all the replicas, while *avoiding causality violations*.

The problem is resolved by the handshake algorithm which merges subtrees of replicas.
It infuses ops that are missing on the other side and sync continues in the steady-state mode from that point on.
Handshakes build the spanning tree out of two-way subscriptions.
The inner workings of a Swarm handshake will be discussed in the 4/5.

At this point, we had a bird-eye view of the Swarm system:
Ops prapagate by a Spanning Tree to all the Replicas to make their States converge.

The next part 3/3 will explain what constitutes a Swarm node and how its parts work together: storage engine, op routing and API objects.



